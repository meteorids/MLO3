{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8450b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import h5py\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import pickle\n",
    "\n",
    "from numpy import asarray\n",
    "from sklearn.preprocessing import MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34beca0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training dataset...\n",
      "Training dataset loaded.\n",
      "[[ 0.8569772   0.06923168 -0.68082735]\n",
      " [ 0.5412468   0.15719147 -0.47982009]\n",
      " [ 1.         -0.28762272 -0.75698736]\n",
      " ...\n",
      " [ 0.57431226 -0.26606853 -0.17039112]\n",
      " [ 0.69803046 -0.0529784  -0.63546659]\n",
      " [ 0.36361063 -0.27315794 -0.52324134]]\n",
      "Iteration 1, loss = 0.04018046\n",
      "Validation score: 0.941733\n",
      "Iteration 2, loss = 0.00320715\n",
      "Validation score: 0.976215\n",
      "Iteration 3, loss = 0.00171073\n",
      "Validation score: 0.983555\n",
      "Iteration 4, loss = 0.00137108\n",
      "Validation score: 0.985729\n",
      "Iteration 5, loss = 0.00122982\n",
      "Validation score: 0.987455\n",
      "Iteration 6, loss = 0.00108804\n",
      "Validation score: 0.988919\n",
      "Iteration 7, loss = 0.00094192\n",
      "Validation score: 0.990021\n",
      "Iteration 8, loss = 0.00079960\n",
      "Validation score: 0.992248\n",
      "Iteration 9, loss = 0.00063597\n",
      "Validation score: 0.993818\n",
      "Iteration 10, loss = 0.00050634\n",
      "Validation score: 0.994885\n",
      "Iteration 11, loss = 0.00043486\n",
      "Validation score: 0.995321\n",
      "Iteration 12, loss = 0.00039254\n",
      "Validation score: 0.995792\n",
      "Iteration 13, loss = 0.00036561\n",
      "Validation score: 0.995935\n",
      "Iteration 14, loss = 0.00034651\n",
      "Validation score: 0.996175\n",
      "Iteration 15, loss = 0.00032720\n",
      "Validation score: 0.996222\n",
      "Iteration 16, loss = 0.00031937\n",
      "Validation score: 0.996486\n",
      "Iteration 17, loss = 0.00031097\n",
      "Validation score: 0.996407\n",
      "Iteration 18, loss = 0.00029718\n",
      "Validation score: 0.996675\n",
      "Iteration 19, loss = 0.00028427\n",
      "Validation score: 0.996705\n",
      "Iteration 20, loss = 0.00026461\n",
      "Validation score: 0.996914\n",
      "Iteration 21, loss = 0.00024855\n",
      "Validation score: 0.997325\n",
      "Iteration 22, loss = 0.00023264\n",
      "Validation score: 0.997469\n",
      "Iteration 23, loss = 0.00022232\n",
      "Validation score: 0.997604\n",
      "Iteration 24, loss = 0.00020485\n",
      "Validation score: 0.997360\n",
      "Iteration 25, loss = 0.00020200\n",
      "Validation score: 0.997782\n",
      "Iteration 26, loss = 0.00021048\n",
      "Validation score: 0.997798\n",
      "Iteration 27, loss = 0.00019629\n",
      "Validation score: 0.997809\n",
      "Iteration 28, loss = 0.00019403\n",
      "Validation score: 0.997985\n",
      "Iteration 29, loss = 0.00017843\n",
      "Validation score: 0.997781\n",
      "Iteration 30, loss = 0.00017787\n",
      "Validation score: 0.997730\n",
      "Iteration 31, loss = 0.00017537\n",
      "Validation score: 0.998095\n",
      "Iteration 32, loss = 0.00017521\n",
      "Validation score: 0.997776\n",
      "Iteration 33, loss = 0.00017576\n",
      "Validation score: 0.997157\n",
      "Iteration 34, loss = 0.00018817\n",
      "Validation score: 0.998246\n",
      "Iteration 35, loss = 0.00019321\n",
      "Validation score: 0.997809\n",
      "Iteration 36, loss = 0.00017897\n",
      "Validation score: 0.998341\n",
      "Iteration 37, loss = 0.00016370\n",
      "Validation score: 0.998265\n",
      "Iteration 38, loss = 0.00016373\n",
      "Validation score: 0.998128\n",
      "Iteration 39, loss = 0.00016140\n",
      "Validation score: 0.998432\n",
      "Iteration 40, loss = 0.00017222\n",
      "Validation score: 0.998340\n",
      "Iteration 41, loss = 0.00017497\n",
      "Validation score: 0.998356\n",
      "Iteration 42, loss = 0.00015273\n",
      "Validation score: 0.998261\n",
      "Iteration 43, loss = 0.00014279\n",
      "Validation score: 0.998240\n",
      "Iteration 44, loss = 0.00017393\n",
      "Validation score: 0.998643\n",
      "Iteration 45, loss = 0.00018053\n",
      "Validation score: 0.998415\n",
      "Iteration 46, loss = 0.00013526\n",
      "Validation score: 0.998650\n",
      "Iteration 47, loss = 0.00016843\n",
      "Validation score: 0.998587\n",
      "Iteration 48, loss = 0.00013851\n",
      "Validation score: 0.998587\n",
      "Iteration 49, loss = 0.00014213\n",
      "Validation score: 0.998655\n",
      "Iteration 50, loss = 0.00015041\n",
      "Validation score: 0.998539\n",
      "Iteration 51, loss = 0.00016000\n",
      "Validation score: 0.998297\n",
      "Iteration 52, loss = 0.00014041\n",
      "Validation score: 0.998574\n",
      "Iteration 53, loss = 0.00014272\n",
      "Validation score: 0.998342\n",
      "Iteration 54, loss = 0.00015556\n",
      "Validation score: 0.998685\n",
      "Iteration 55, loss = 0.00013136\n",
      "Validation score: 0.998452\n",
      "Iteration 56, loss = 0.00014143\n",
      "Validation score: 0.998790\n",
      "Iteration 57, loss = 0.00016438\n",
      "Validation score: 0.997436\n",
      "Iteration 58, loss = 0.00012678\n",
      "Validation score: 0.998645\n",
      "Iteration 59, loss = 0.00013642\n",
      "Validation score: 0.998780\n",
      "Iteration 60, loss = 0.00013221\n",
      "Validation score: 0.998508\n",
      "Iteration 61, loss = 0.00013087\n",
      "Validation score: 0.998317\n",
      "Iteration 62, loss = 0.00015188\n",
      "Validation score: 0.998703\n",
      "Iteration 63, loss = 0.00013216\n",
      "Validation score: 0.998899\n",
      "Iteration 64, loss = 0.00013104\n",
      "Validation score: 0.998580\n",
      "Iteration 65, loss = 0.00013149\n",
      "Validation score: 0.998573\n",
      "Iteration 66, loss = 0.00014420\n",
      "Validation score: 0.997865\n",
      "Iteration 67, loss = 0.00014085\n",
      "Validation score: 0.998851\n",
      "Iteration 68, loss = 0.00011934\n",
      "Validation score: 0.998258\n",
      "Iteration 69, loss = 0.00012616\n",
      "Validation score: 0.998730\n",
      "Iteration 70, loss = 0.00012609\n",
      "Validation score: 0.998968\n",
      "Iteration 71, loss = 0.00012295\n",
      "Validation score: 0.998823\n",
      "Iteration 72, loss = 0.00012026\n",
      "Validation score: 0.998963\n",
      "Iteration 73, loss = 0.00013503\n",
      "Validation score: 0.998002\n",
      "Iteration 74, loss = 0.00014040\n",
      "Validation score: 0.998291\n",
      "Iteration 75, loss = 0.00012410\n",
      "Validation score: 0.998642\n",
      "Iteration 76, loss = 0.00012428\n",
      "Validation score: 0.998565\n",
      "Iteration 77, loss = 0.00012197\n",
      "Validation score: 0.998916\n",
      "Iteration 78, loss = 0.00011980\n",
      "Validation score: 0.998882\n",
      "Iteration 79, loss = 0.00011156\n",
      "Validation score: 0.998460\n",
      "Iteration 80, loss = 0.00012735\n",
      "Validation score: 0.998935\n",
      "Iteration 81, loss = 0.00011278\n",
      "Validation score: 0.999046\n",
      "Iteration 82, loss = 0.00011800\n",
      "Validation score: 0.998981\n",
      "Iteration 83, loss = 0.00011383\n",
      "Validation score: 0.998526\n",
      "Iteration 84, loss = 0.00012467\n",
      "Validation score: 0.998935\n",
      "Iteration 85, loss = 0.00010936\n",
      "Validation score: 0.998940\n",
      "Iteration 86, loss = 0.00011585\n",
      "Validation score: 0.998842\n",
      "Iteration 87, loss = 0.00012933\n",
      "Validation score: 0.999072\n",
      "Iteration 88, loss = 0.00011293\n",
      "Validation score: 0.998724\n",
      "Iteration 89, loss = 0.00011723\n",
      "Validation score: 0.998334\n",
      "Iteration 90, loss = 0.00010947\n",
      "Validation score: 0.999072\n",
      "Iteration 91, loss = 0.00011865\n",
      "Validation score: 0.998862\n",
      "Iteration 92, loss = 0.00010888\n",
      "Validation score: 0.998845\n",
      "Iteration 93, loss = 0.00012535\n",
      "Validation score: 0.999028\n",
      "Iteration 94, loss = 0.00010616\n",
      "Validation score: 0.998985\n",
      "Iteration 95, loss = 0.00013098\n",
      "Validation score: 0.998875\n",
      "Iteration 96, loss = 0.00011019\n",
      "Validation score: 0.999146\n",
      "Iteration 97, loss = 0.00011324\n",
      "Validation score: 0.998387\n",
      "Iteration 98, loss = 0.00011228\n",
      "Validation score: 0.998608\n",
      "Iteration 99, loss = 0.00011456\n",
      "Validation score: 0.998849\n",
      "Iteration 100, loss = 0.00010700\n",
      "Validation score: 0.999043\n",
      "Iteration 101, loss = 0.00010661\n",
      "Validation score: 0.999080\n",
      "Iteration 102, loss = 0.00010890\n",
      "Validation score: 0.998794\n",
      "Iteration 103, loss = 0.00010527\n",
      "Validation score: 0.999157\n",
      "Iteration 104, loss = 0.00010683\n",
      "Validation score: 0.999193\n",
      "Iteration 105, loss = 0.00011255\n",
      "Validation score: 0.998979\n",
      "Iteration 106, loss = 0.00010313\n",
      "Validation score: 0.999218\n",
      "Iteration 107, loss = 0.00009585\n",
      "Validation score: 0.999051\n",
      "Iteration 108, loss = 0.00012372\n",
      "Validation score: 0.999168\n",
      "Iteration 109, loss = 0.00010176\n",
      "Validation score: 0.998503\n",
      "Iteration 110, loss = 0.00010015\n",
      "Validation score: 0.999118\n",
      "Iteration 111, loss = 0.00011966\n",
      "Validation score: 0.999152\n",
      "Iteration 112, loss = 0.00009376\n",
      "Validation score: 0.998969\n",
      "Iteration 113, loss = 0.00009632\n",
      "Validation score: 0.999256\n",
      "Iteration 114, loss = 0.00010539\n",
      "Validation score: 0.999160\n",
      "Iteration 115, loss = 0.00011297\n",
      "Validation score: 0.998562\n",
      "Iteration 116, loss = 0.00010929\n",
      "Validation score: 0.998235\n",
      "Iteration 117, loss = 0.00009895\n",
      "Validation score: 0.998860\n",
      "Iteration 118, loss = 0.00010352\n",
      "Validation score: 0.997131\n",
      "Iteration 119, loss = 0.00009883\n",
      "Validation score: 0.998984\n",
      "Iteration 120, loss = 0.00010466\n",
      "Validation score: 0.998338\n",
      "Iteration 121, loss = 0.00012089\n",
      "Validation score: 0.998737\n",
      "Iteration 122, loss = 0.00009988\n",
      "Validation score: 0.999023\n",
      "Iteration 123, loss = 0.00009105\n",
      "Validation score: 0.999252\n",
      "Iteration 124, loss = 0.00011686\n",
      "Validation score: 0.998642\n",
      "Validation score did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# disable FutureWarning\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "train_data_path='./Training_Data/'\n",
    "output_path='./net/'\n",
    "\n",
    "nnstr='Ozone' #NN name string\n",
    "hiddenlayers=(100,90,75) #set MLNN hidden layers (hidden layers will be reset for aaNN based on number of bands)\n",
    "\n",
    "#radoption=2 #TOA rad data type, options: 1=Lt, 2=Lrc \n",
    "addnoise=0 #Flag for adding Gaussian noise NOTE: do not add noise for forward training\n",
    "noiselevel=0 #Gaussian noise level, e.g. 1=1%\n",
    "\n",
    "#sensor band infos\n",
    "band=[302,312,320,340,380]\n",
    "trainband=np.arange(5)\n",
    "nrrs=5\n",
    "aodidx=np.arange(5)\n",
    "\n",
    "#number of sample in the scatter plot, if total number of data points is less than this, half of the data will be used for plotting\n",
    "nplotsample=5000 \n",
    "\t\n",
    "nband=len(band)\n",
    "ntrainband=len(trainband)\n",
    "naod=len(aodidx) \n",
    "      \n",
    "#read in training data\n",
    "print('Loading training dataset...')\n",
    "for i in np.arange(5):\n",
    "    if i==0:       \n",
    "       par=np.loadtxt(train_data_path +'ML-input'+str(i+1)+'.txt')\n",
    "    else:\n",
    "       par=np.append(par,np.loadtxt(train_data_path +'ML-input'+str(i+1)+'.txt'),0)\n",
    "\n",
    "print('Training dataset loaded.')\n",
    "\n",
    "\n",
    "\n",
    "#remove negative values\n",
    "idx=np.where(np.sum(par<0,axis=1)==0)[0]\n",
    "par=par[idx,:]\n",
    "\n",
    "#total number of training cases\n",
    "ncase=len(par)\n",
    "rad=np.zeros((ncase,1))\n",
    "\n",
    "#rad[:,0] = par[:,3]\n",
    "#rad[:,1] = par[:,4]\n",
    "\n",
    "#generate the gaussian noise for each band\n",
    "if addnoise==1:\n",
    "    fname_prefix=nnstr+'WiGN_p'+str(noiselevel)\n",
    "    noise=np.random.normal(1,noiselevel/100,(ncase,1))    # only use 2 ratio\n",
    "    rad=np.multiply(rad,noise) \n",
    "else:\n",
    "    fname_prefix=nnstr+'WoGN' \n",
    "     \n",
    "# NN training: Ozone & Cloud Optical Depth   \n",
    "#trainingoption = itrain + 1\n",
    "\n",
    "nnlayer = hiddenlayers\n",
    "            \n",
    "nlayer=len(nnlayer) # number of hidden layers\n",
    "    \n",
    "#create layer string for file name\n",
    "layerstr=''\n",
    "for i in np.arange(nlayer):\n",
    "    if i<nlayer-1:\n",
    "       layerstr=layerstr+str(nnlayer[i])+'X'\n",
    "    else:\n",
    "       layerstr=layerstr+str(nnlayer[i])\n",
    "\n",
    "trainx=np.zeros((ncase,3))\n",
    "trainy=np.zeros((ncase,2))\n",
    "\n",
    "trainx[:,0]=np.cos(np.deg2rad(par[:,0])) # geometry: cos[Solar Zenith Angle]\n",
    "trainx[:,1]=np.log10(par[:,3]) # Irradiance 380\n",
    "trainx[:,2]=np.log10(par[:,4]) # Ratio\n",
    "\n",
    "trainy[:,0]=np.log10(par[:,1]) # Ozone\n",
    "trainy[:,1]=np.log10(par[:,2]) # Cloud. Vol. Frac.\n",
    "\n",
    "net_name='net_'+fname_prefix+'_ozone'+str(nrrs)+'_'+layerstr+'.h5'\t  \n",
    "for i in np.arange(nrrs):\n",
    "    if i==0:\n",
    "       labelparam=['Rrs'+str(band[i])+'nm']\n",
    "    else:\n",
    "       labelparam=np.append(labelparam,['Rrs'+str(band[i])+'nm'],0)\t\t\t  \n",
    "\n",
    "ninput=len(trainx[0])\n",
    "train_in=np.zeros((ninput,2))\n",
    "for i in range(ninput):\n",
    "    train_in[i,0]=trainx[:,i].min()\n",
    "    train_in[i,1]=trainx[:,i].max()\n",
    "noutput=len(trainy[0])\t\n",
    "train_out=np.zeros((noutput,2))\n",
    "for i in range(noutput):\n",
    "     train_out[i,0]=trainy[:,i].min()\n",
    "     train_out[i,1]=trainy[:,i].max()\n",
    "    \n",
    "#normalize the trainx and trainy to [-1,1]\t\n",
    "for i in range(ninput):\n",
    "    trainx[:,i]=2*(trainx[:,i]-train_in[i,0])/(train_in[i,1]-train_in[i,0])-1\n",
    "for i in range(noutput):\n",
    "    trainy[:,i]=2*(trainy[:,i]-train_out[i,0])/(train_out[i,1]-train_out[i,0])-1\n",
    "\n",
    "#scaler = MinMaxScaler()\n",
    "# transform data\n",
    "#scaled_train = scaler.fit_transform(trainx)\n",
    "\n",
    "\n",
    "print(trainx)\n",
    "\n",
    "\n",
    "#Build MLNN\n",
    "mlnn=MLPRegressor(hidden_layer_sizes=nnlayer,\n",
    "     activation='tanh',\n",
    "\t  solver='adam',\n",
    "    \t  batch_size='auto',\n",
    "    \t  learning_rate='adaptive',\n",
    "    \t  learning_rate_init=0.001,\n",
    "    \t  max_iter=1000,\n",
    "    \t  random_state=5,\n",
    "    \t  tol=1.0e-8,\n",
    "    \t  verbose=True,\n",
    "    \t  early_stopping=True,  \n",
    "    \t  validation_fraction=0.1)\n",
    "#traing MLNN\n",
    "mlnn=mlnn.fit(trainx,trainy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07e7c609",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(mlnn, open('net/model1-ozone', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "452a1c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1Ozone = pickle.load(open('net/model1-ozone', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57872a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.73076923 -0.7460093  -0.9919579 ]\n",
      " [-0.30769231 -0.68764076 -0.9806662 ]\n",
      " [-1.         -0.89682972 -0.99474269]\n",
      " ...\n",
      " [ 0.03846154 -0.85905802 -0.99218652]\n",
      " [-0.80769231 -0.08738813 -0.99972213]\n",
      " [ 0.84615385 -0.91706171 -0.78388452]]\n"
     ]
    }
   ],
   "source": [
    "validationdata =  np.loadtxt('Data/validation1.txt')\n",
    "\n",
    "#normalizing input data\n",
    "\n",
    "\n",
    "ninput=len(validationdata[0])\n",
    "train_in=np.zeros((ninput,2))\n",
    "for i in range(ninput):\n",
    "    train_in[i,0]=validationdata[:,i].min()\n",
    "    train_in[i,1]=validationdata[:,i].max()\n",
    "    \n",
    "    \n",
    "#noutput=len(trainy[0])\t\n",
    "#train_out=np.zeros((noutput,2))\n",
    "#for i in range(noutput):\n",
    "#    train_out[i,0]=trainy[:,i].min()\n",
    "#    train_out[i,1]=trainy[:,i].max()\n",
    "\n",
    "\n",
    "for i in range(ninput):\n",
    "    validationdata[:,i]=2*(validationdata[:,i]-train_in[i,0])/(train_in[i,1]-train_in[i,0])-1\n",
    "\n",
    "print(validationdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c225f86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#type(validationdata)\n",
    "\n",
    "#validationdata.shape, trainx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50f75671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of a normalization\n",
    "#from numpy import asarray\n",
    "#from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "# define min max scaler\n",
    "#scaler = MinMaxScaler()\n",
    "# transform data\n",
    "#scaled_train = scaler.fit_transform(trainx)\n",
    "\n",
    "#print(scaled_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d9850ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaled_validation = scaler.transform(validationdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "930b7b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model1Ozone.predict(validationdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5d14c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.35640428  7.5405636 ]\n",
      " [ 0.39845905  9.82967838]\n",
      " [ 0.33298244  7.70309216]\n",
      " ...\n",
      " [ 0.40903445 17.83949309]\n",
      " [ 0.34488264  1.43242876]\n",
      " [ 0.58644547 31.48156189]]\n"
     ]
    }
   ],
   "source": [
    "nnoutput = model1Ozone.predict(validationdata)\n",
    "\n",
    "\n",
    "for i in range(noutput):\n",
    "    nnoutput[:,i]=(nnoutput[:,i]+1)/2*(train_out[i,1]-train_out[i,0])+train_out[i,0]\n",
    "    trainy[:,i]=(trainy[:,i]+1)/2*(train_out[i,1]-train_out[i,0])+train_out[i,0]\n",
    "\t\n",
    "nnoutput=10 ** nnoutput\n",
    "trainy=10 ** trainy\n",
    "\n",
    "print(nnoutput)\n",
    "\n",
    "#print(trainy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50476515",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "   \n",
    "#compute average percentage error\n",
    "diff=(nnoutput-trainy)/trainy*100\n",
    "ape=np.mean(np.absolute(diff), axis=0)\n",
    "bias=np.mean(diff, axis=0)\n",
    "    \n",
    "lim=np.amax(trainy,axis=0)*1.2 #set limit\n",
    "r2=np.zeros(noutput)\n",
    "if ncase>nplotsample:\n",
    "    idx=random.sample(list(range(ncase)),nplotsample)\n",
    "else:\n",
    "    idx=random.sample(list(range(ncase)),int(ncase/2))\n",
    "if noutput<5:\n",
    "    plt.figure(figsize=(18,5),dpi=150)\n",
    "else:\n",
    "    plt.figure(figsize=(18,9),dpi=150)\n",
    "for i in np.arange(noutput):\n",
    "    r2[i]=r2_score(trainy[:,i],nnoutput[:,i])\n",
    "    print(r2[i])\n",
    "    plt.subplot(2,4,i+1)           \n",
    "    plt.scatter(trainy[idx,i],nnoutput[idx,i],s=2,c='red')\n",
    "    plt.xlim(0, lim[i])\n",
    "    plt.ylim(0, lim[i])\n",
    "    plt.xlabel('Model '+labelparam[i]) #original line:   plt.xlabel('Model '+labelparam[i])\n",
    "    plt.ylabel('MLNN '+labelparam[i])  #original line: plt.ylabel('MLNN '+labelparam[i])\n",
    "    plt.plot([0,lim[i]],[0,lim[i]],'k')\n",
    "    plt.text(lim[i]*0.05,lim[i]*0.94,('R$^2$ = %0.3f' % (r2[i])))\n",
    "    plt.text(lim[i]*0.05,lim[i]*0.88,('APE = %0.2f' % (ape[i])+'%'))\n",
    "    plt.text(lim[i]*0.05,lim[i]*0.82,('Bias = %0.2f' % (bias[i])+'%'))\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_path+net_name+'_Training.png')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
