{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26b7d19b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training dataset...\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "./Training_Data/ML-input2.txt not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-bd06503ace86>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m        \u001b[0mpar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_path\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m'NILU-training'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m        \u001b[0mpar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_path\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m'ML-input'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Training dataset loaded.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mloadtxt\u001b[0;34m(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin, encoding, max_rows)\u001b[0m\n\u001b[1;32m    966\u001b[0m             \u001b[0mfname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_string_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 968\u001b[0;31m             \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_datasource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    969\u001b[0m             \u001b[0mfencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'encoding'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'latin1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m             \u001b[0mfh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/numpy/lib/_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(path, mode, destpath, encoding, newline)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataSource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdestpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnewline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/numpy/lib/_datasource.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, path, mode, encoding, newline)\u001b[0m\n\u001b[1;32m    621\u001b[0m                                       encoding=encoding, newline=newline)\n\u001b[1;32m    622\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 623\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s not found.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: ./Training_Data/ML-input2.txt not found."
     ]
    }
   ],
   "source": [
    "\n",
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import h5py\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "# disable FutureWarning\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "train_data_path='./Training_Data/'\n",
    "output_path='./net/'\n",
    "\n",
    "nnstr='Ozone' #NN name string\n",
    "hiddenlayers=(100,90,75) #set MLNN hidden layers (hidden layers will be reset for aaNN based on number of bands)\n",
    "\n",
    "#radoption=2 #TOA rad data type, options: 1=Lt, 2=Lrc \n",
    "addnoise=0 #Flag for adding Gaussian noise NOTE: do not add noise for forward training\n",
    "noiselevel=0 #Gaussian noise level, e.g. 1=1%\n",
    "\n",
    "#sensor band infos\n",
    "band=[302,312,320,340,380]\n",
    "trainband=np.arange(5)\n",
    "nrrs=5\n",
    "aodidx=np.arange(5)\n",
    "\n",
    "#number of sample in the scatter plot, if total number of data points is less than this, half of the data will be used for plotting\n",
    "nplotsample=5000 \n",
    "\t\n",
    "nband=len(band)\n",
    "ntrainband=len(trainband)\n",
    "naod=len(aodidx) \n",
    "      \n",
    "#read in training data\n",
    "print('Loading training dataset...')\n",
    "for i in np.arange(5):\n",
    "    if i==0:       \n",
    "       par=np.loadtxt(train_data_path +'NILU-training'+str(i+1)+'.txt')\n",
    "    else:\n",
    "       par=np.append(par,np.loadtxt(train_data_path +'NILU-training'+str(i+1)+'.txt'),0)\n",
    "\n",
    "print('Training dataset loaded.')\n",
    "\n",
    "\n",
    "\n",
    "#remove negative values\n",
    "idx=np.where(np.sum(par<0,axis=1)==0)[0]\n",
    "par=par[idx,:]\n",
    "\n",
    "#total number of training cases\n",
    "ncase=len(par)\n",
    "rad=np.zeros((ncase,1))\n",
    "\n",
    "#rad[:,0] = par[:,3]\n",
    "#rad[:,1] = par[:,4]\n",
    "\n",
    "#generate the gaussian noise for each band\n",
    "if addnoise==1:\n",
    "    fname_prefix=nnstr+'WiGN_p'+str(noiselevel)\n",
    "    noise=np.random.normal(1,noiselevel/100,(ncase,1))    # only use 2 ratio\n",
    "    rad=np.multiply(rad,noise) \n",
    "else:\n",
    "    fname_prefix=nnstr+'WoGN' \n",
    "     \n",
    "# NN training: Ozone & Cloud Optical Depth   \n",
    "#trainingoption = itrain + 1\n",
    "\n",
    "nnlayer = hiddenlayers\n",
    "            \n",
    "nlayer=len(nnlayer) # number of hidden layers\n",
    "    \n",
    "#create layer string for file name\n",
    "layerstr=''\n",
    "for i in np.arange(nlayer):\n",
    "    if i<nlayer-1:\n",
    "       layerstr=layerstr+str(nnlayer[i])+'X'\n",
    "    else:\n",
    "       layerstr=layerstr+str(nnlayer[i])\n",
    "\n",
    "trainx=np.zeros((ncase,3))\n",
    "trainy=np.zeros((ncase,2))\n",
    "\n",
    "trainx[:,0]=np.cos(np.deg2rad(par[:,0])) # geometry: cos[Solar Zenith Angle]\n",
    "trainx[:,1]=np.log10(par[:,3]) # Irradiance 380\n",
    "trainx[:,2]=np.log10(par[:,4]) # Ratio\n",
    "\n",
    "trainy[:,0]=np.log10(par[:,1]) # Ozone\n",
    "trainy[:,1]=np.log10(par[:,2]) # Cloud. Vol. Frac.\n",
    "\n",
    "net_name='net_'+fname_prefix+'_ozone'+str(nrrs)+'_'+layerstr+'.h5'\t  \n",
    "for i in np.arange(nrrs):\n",
    "    if i==0:\n",
    "       labelparam=['Rrs'+str(band[i])+'nm']\n",
    "    else:\n",
    "       labelparam=np.append(labelparam,['Rrs'+str(band[i])+'nm'],0)\t\t\t  \n",
    "\n",
    "ninput=len(trainx[0])\n",
    "train_in=np.zeros((ninput,2))\n",
    "for i in range(ninput):\n",
    "    train_in[i,0]=trainx[:,i].min()\n",
    "    train_in[i,1]=trainx[:,i].max()\n",
    "noutput=len(trainy[0])\t\n",
    "train_out=np.zeros((noutput,2))\n",
    "for i in range(noutput):\n",
    "    train_out[i,0]=trainy[:,i].min()\n",
    "    train_out[i,1]=trainy[:,i].max()\n",
    "#normalize the trainx and trainy to [-1,1]\t\n",
    "for i in range(ninput):\n",
    "    trainx[:,i]=2*(trainx[:,i]-train_in[i,0])/(train_in[i,1]-train_in[i,0])-1\n",
    "for i in range(noutput):\n",
    "    trainy[:,i]=2*(trainy[:,i]-train_out[i,0])/(train_out[i,1]-train_out[i,0])-1\n",
    "\t\n",
    "#Build MLNN\n",
    "mlnn=MLPRegressor(hidden_layer_sizes=nnlayer,\n",
    "     activation='tanh',\n",
    "\t  solver='adam',\n",
    "    \t  batch_size='auto',\n",
    "    \t  learning_rate='adaptive',\n",
    "    \t  learning_rate_init=0.001,\n",
    "    \t  max_iter=1000,\n",
    "    \t  random_state=5,\n",
    "    \t  tol=1.0e-8,\n",
    "    \t  verbose=True,\n",
    "    \t  early_stopping=True,  \n",
    "    \t  validation_fraction=0.1)\n",
    "#traing MLNN\n",
    "mlnn=mlnn.fit(trainx,trainy)\n",
    "    \n",
    "#save trained NN to HDF5\n",
    "nn_structure=np.zeros([nlayer+2])\n",
    "nn_structure[0]=ninput\n",
    "for i in range(nlayer):\n",
    "    nn_structure[i+1]=nnlayer[i]\n",
    "nn_structure[nlayer+1]=noutput\n",
    "    \n",
    "hf = h5py.File('./net/'+net_name,'w')\n",
    "hf.create_dataset('Layers',dtype='int8',data = nn_structure)\n",
    "hf.create_dataset('Norm_in',dtype='float64',data = train_in)\n",
    "hf.create_dataset('Norm_out',dtype='float64',data = train_out)\n",
    "gw = hf.create_group('Weights')\n",
    "for i in range(nlayer+1):    \n",
    "    gw.create_dataset('Layer'+str(i+1),dtype='float64',data=np.transpose(mlnn.coefs_[i]))\n",
    "gb = hf.create_group('Bias')\n",
    "for i in range(nlayer+1):\n",
    "    gb.create_dataset('Layer'+str(i+1),dtype='float64',data=mlnn.intercepts_[i].reshape(int(nn_structure[i+1]),1))\n",
    "hf.close()   \n",
    "\n",
    "#make prediction using trained MLNN\n",
    "nnoutput=mlnn.predict(trainx)\n",
    "#converting data\n",
    "for i in range(noutput):\n",
    "    nnoutput[:,i]=(nnoutput[:,i]+1)/2*(train_out[i,1]-train_out[i,0])+train_out[i,0]\n",
    "    trainy[:,i]=(trainy[:,i]+1)/2*(train_out[i,1]-train_out[i,0])+train_out[i,0]\n",
    "\t\n",
    "nnoutput=10 ** nnoutput\n",
    "trainy=10 ** trainy\n",
    "    \n",
    "#compute average percentage error\n",
    "diff=(nnoutput-trainy)/trainy*100\n",
    "ape=np.mean(np.absolute(diff), axis=0)\n",
    "bias=np.mean(diff, axis=0)\n",
    "    \n",
    "lim=np.amax(trainy,axis=0)*1.2 #set limit\n",
    "r2=np.zeros(noutput)\n",
    "if ncase>nplotsample:\n",
    "    idx=random.sample(list(range(ncase)),nplotsample)\n",
    "else:\n",
    "    idx=random.sample(list(range(ncase)),int(ncase/2))\n",
    "if noutput<5:\n",
    "    plt.figure(figsize=(18,5),dpi=150)\n",
    "else:\n",
    "    plt.figure(figsize=(18,9),dpi=150)\n",
    "for i in np.arange(noutput):\n",
    "    r2[i]=r2_score(trainy[:,i],nnoutput[:,i])\n",
    "    print(r2[i])\n",
    "    plt.subplot(2,4,i+1)           \n",
    "    plt.scatter(trainy[idx,i],nnoutput[idx,i],s=2,c='red')\n",
    "    plt.xlim(0, lim[i])\n",
    "    plt.ylim(0, lim[i])\n",
    "    plt.xlabel('Model '+labelparam[i]) #original line:   plt.xlabel('Model '+labelparam[i])\n",
    "    plt.ylabel('MLNN '+labelparam[i])  #original line: plt.ylabel('MLNN '+labelparam[i])\n",
    "    plt.plot([0,lim[i]],[0,lim[i]],'k')\n",
    "    plt.text(lim[i]*0.05,lim[i]*0.94,('R$^2$ = %0.3f' % (r2[i])))\n",
    "    plt.text(lim[i]*0.05,lim[i]*0.88,('APE = %0.2f' % (ape[i])+'%'))\n",
    "    plt.text(lim[i]*0.05,lim[i]*0.82,('Bias = %0.2f' % (bias[i])+'%'))\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_path+net_name+'_Training.png')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13143828",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
