{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bba35a91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading training dataset...\n",
      "Training dataset loaded.\n",
      "Iteration 1, loss = 0.07748353\n",
      "Validation score: 0.379451\n",
      "Iteration 2, loss = 0.01765189\n",
      "Validation score: 0.828511\n",
      "Iteration 3, loss = 0.00595156\n",
      "Validation score: 0.895724\n",
      "Iteration 4, loss = 0.00393704\n",
      "Validation score: 0.921797\n",
      "Iteration 5, loss = 0.00355560\n",
      "Validation score: 0.928002\n",
      "Iteration 6, loss = 0.00337564\n",
      "Validation score: 0.932809\n",
      "Iteration 7, loss = 0.00320248\n",
      "Validation score: 0.916966\n",
      "Iteration 8, loss = 0.00325115\n",
      "Validation score: 0.919757\n",
      "Iteration 9, loss = 0.00307214\n",
      "Validation score: 0.926390\n",
      "Iteration 10, loss = 0.00300741\n",
      "Validation score: 0.938215\n",
      "Iteration 11, loss = 0.00318157\n",
      "Validation score: 0.938950\n",
      "Iteration 12, loss = 0.00285541\n",
      "Validation score: 0.943254\n",
      "Iteration 13, loss = 0.00296682\n",
      "Validation score: 0.931969\n",
      "Iteration 14, loss = 0.00265011\n",
      "Validation score: 0.944858\n",
      "Iteration 15, loss = 0.00281493\n",
      "Validation score: 0.949563\n",
      "Iteration 16, loss = 0.00245937\n",
      "Validation score: 0.940057\n",
      "Iteration 17, loss = 0.00244832\n",
      "Validation score: 0.913494\n",
      "Iteration 18, loss = 0.00227084\n",
      "Validation score: 0.949448\n",
      "Iteration 19, loss = 0.00224660\n",
      "Validation score: 0.958664\n",
      "Iteration 20, loss = 0.00192044\n",
      "Validation score: 0.959345\n",
      "Iteration 21, loss = 0.00193868\n",
      "Validation score: 0.962872\n",
      "Iteration 22, loss = 0.00167368\n",
      "Validation score: 0.962297\n",
      "Iteration 23, loss = 0.00196033\n",
      "Validation score: 0.964476\n",
      "Iteration 24, loss = 0.00174860\n",
      "Validation score: 0.967836\n",
      "Iteration 25, loss = 0.00151508\n",
      "Validation score: 0.967046\n",
      "Iteration 26, loss = 0.00151356\n",
      "Validation score: 0.970330\n",
      "Iteration 27, loss = 0.00139152\n",
      "Validation score: 0.969469\n",
      "Iteration 28, loss = 0.00142999\n",
      "Validation score: 0.971078\n",
      "Iteration 29, loss = 0.00135453\n",
      "Validation score: 0.966284\n",
      "Iteration 30, loss = 0.00133716\n",
      "Validation score: 0.971984\n",
      "Iteration 31, loss = 0.00128912\n",
      "Validation score: 0.972248\n",
      "Iteration 32, loss = 0.00144474\n",
      "Validation score: 0.972492\n",
      "Iteration 33, loss = 0.00132751\n",
      "Validation score: 0.973096\n",
      "Iteration 34, loss = 0.00123375\n",
      "Validation score: 0.974522\n",
      "Iteration 35, loss = 0.00117833\n",
      "Validation score: 0.977191\n",
      "Iteration 36, loss = 0.00113648\n",
      "Validation score: 0.976566\n",
      "Iteration 37, loss = 0.00117464\n",
      "Validation score: 0.975599\n",
      "Iteration 38, loss = 0.00125096\n",
      "Validation score: 0.973487\n",
      "Iteration 39, loss = 0.00123666\n",
      "Validation score: 0.979839\n",
      "Iteration 40, loss = 0.00109802\n",
      "Validation score: 0.966611\n",
      "Iteration 41, loss = 0.00093545\n",
      "Validation score: 0.975600\n",
      "Iteration 42, loss = 0.00105528\n",
      "Validation score: 0.980959\n",
      "Iteration 43, loss = 0.00101226\n",
      "Validation score: 0.981194\n",
      "Iteration 44, loss = 0.00092995\n",
      "Validation score: 0.982749\n",
      "Iteration 45, loss = 0.00090387\n",
      "Validation score: 0.980875\n",
      "Iteration 46, loss = 0.00088398\n",
      "Validation score: 0.980758\n",
      "Iteration 47, loss = 0.00077578\n",
      "Validation score: 0.983140\n",
      "Iteration 48, loss = 0.00090128\n",
      "Validation score: 0.983331\n",
      "Iteration 49, loss = 0.00082711\n",
      "Validation score: 0.979774\n",
      "Iteration 50, loss = 0.00078270\n",
      "Validation score: 0.984339\n",
      "Iteration 51, loss = 0.00071129\n",
      "Validation score: 0.985427\n",
      "Iteration 52, loss = 0.00084013\n",
      "Validation score: 0.985731\n",
      "Iteration 53, loss = 0.00116944\n",
      "Validation score: 0.966741\n",
      "Iteration 54, loss = 0.00075967\n",
      "Validation score: 0.986357\n",
      "Iteration 55, loss = 0.00080106\n",
      "Validation score: 0.982417\n",
      "Iteration 56, loss = 0.00090044\n",
      "Validation score: 0.985440\n",
      "Iteration 57, loss = 0.00066220\n",
      "Validation score: 0.955228\n",
      "Iteration 58, loss = 0.00080010\n",
      "Validation score: 0.976080\n",
      "Iteration 59, loss = 0.00093081\n",
      "Validation score: 0.984043\n",
      "Iteration 60, loss = 0.00073158\n",
      "Validation score: 0.982633\n",
      "Iteration 61, loss = 0.00061457\n",
      "Validation score: 0.987300\n",
      "Iteration 62, loss = 0.00063884\n",
      "Validation score: 0.987568\n",
      "Iteration 63, loss = 0.00071654\n",
      "Validation score: 0.986999\n",
      "Iteration 64, loss = 0.00067469\n",
      "Validation score: 0.986893\n",
      "Iteration 65, loss = 0.00073953\n",
      "Validation score: 0.979801\n",
      "Iteration 66, loss = 0.00079555\n",
      "Validation score: 0.960721\n",
      "Iteration 67, loss = 0.00094285\n",
      "Validation score: 0.987162\n",
      "Iteration 68, loss = 0.00063418\n",
      "Validation score: 0.984134\n",
      "Iteration 69, loss = 0.00063452\n",
      "Validation score: 0.986628\n",
      "Iteration 70, loss = 0.00068126\n",
      "Validation score: 0.986924\n",
      "Iteration 71, loss = 0.00056015\n",
      "Validation score: 0.988701\n",
      "Iteration 72, loss = 0.00069284\n",
      "Validation score: 0.986915\n",
      "Iteration 73, loss = 0.00060935\n",
      "Validation score: 0.977311\n",
      "Iteration 74, loss = 0.00059668\n",
      "Validation score: 0.988115\n",
      "Iteration 75, loss = 0.00070768\n",
      "Validation score: 0.974512\n",
      "Iteration 76, loss = 0.00063664\n",
      "Validation score: 0.988909\n",
      "Iteration 77, loss = 0.00073689\n",
      "Validation score: 0.973644\n",
      "Iteration 78, loss = 0.00068658\n",
      "Validation score: 0.984248\n",
      "Iteration 79, loss = 0.00056807\n",
      "Validation score: 0.986020\n",
      "Iteration 80, loss = 0.00060682\n",
      "Validation score: 0.975086\n",
      "Iteration 81, loss = 0.00063609\n",
      "Validation score: 0.983984\n",
      "Iteration 82, loss = 0.00056100\n",
      "Validation score: 0.989874\n",
      "Iteration 83, loss = 0.00068179\n",
      "Validation score: 0.985600\n",
      "Iteration 84, loss = 0.00064509\n",
      "Validation score: 0.976510\n",
      "Iteration 85, loss = 0.00050819\n",
      "Validation score: 0.989134\n",
      "Iteration 86, loss = 0.00058049\n",
      "Validation score: 0.980096\n",
      "Iteration 87, loss = 0.00064914\n",
      "Validation score: 0.984134\n",
      "Iteration 88, loss = 0.00062247\n",
      "Validation score: 0.984460\n",
      "Iteration 89, loss = 0.00062440\n",
      "Validation score: 0.969969\n",
      "Iteration 90, loss = 0.00052976\n",
      "Validation score: 0.987128\n",
      "Iteration 91, loss = 0.00064152\n",
      "Validation score: 0.983540\n",
      "Iteration 92, loss = 0.00058010\n",
      "Validation score: 0.975966\n",
      "Iteration 93, loss = 0.00081303\n",
      "Validation score: 0.982571\n",
      "Validation score did not improve more than tol=0.000000 for 10 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "#milostipanov@gmail.com 2022\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import h5py\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "# disable FutureWarning\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "train_data_path='./Training_Data/'\n",
    "output_path='./net/'\n",
    "\n",
    "nnstr='Ozone' #NN name string\n",
    "hiddenlayers=(100,90,75) #set MLNN hidden layers (hidden layers will be reset for aaNN based on number of bands)\n",
    "\n",
    "#radoption=2 #TOA rad data type, options: 1=Lt, 2=Lrc \n",
    "addnoise=0 #Flag for adding Gaussian noise NOTE: do not add noise for forward training\n",
    "noiselevel=0 #Gaussian noise level, e.g. 1=1%\n",
    "\n",
    "#sensor band infos\n",
    "band=[302,312,320,340,380]\n",
    "trainband=np.arange(5)\n",
    "nrrs=5\n",
    "aodidx=np.arange(5)\n",
    "\n",
    "#number of sample in the scatter plot, if total number of data points is less than this, half of the data will be used for plotting\n",
    "nplotsample=5000 \n",
    "\t\n",
    "nband=len(band)\n",
    "ntrainband=len(trainband)\n",
    "naod=len(aodidx) \n",
    "      \n",
    "#read in training data\n",
    "print('Loading training dataset...')\n",
    "for i in np.arange(5):\n",
    "    if i==0:       \n",
    "       par=np.loadtxt(train_data_path +'NILU-training'+str(i+1)+'.txt')\n",
    "    else:\n",
    "       par=np.append(par,np.loadtxt(train_data_path +'NILU-training'+str(i+1)+'.txt'),0)\n",
    "\n",
    "print('Training dataset loaded.')\n",
    "\n",
    "\n",
    "\n",
    "#remove negative values\n",
    "idx=np.where(np.sum(par<0,axis=1)==0)[0]\n",
    "par=par[idx,:]\n",
    "\n",
    "#total number of training cases\n",
    "ncase=len(par)\n",
    "rad=np.zeros((ncase,1))\n",
    "\n",
    "#rad[:,0] = par[:,3]\n",
    "#rad[:,1] = par[:,4]\n",
    "\n",
    "#generate the gaussian noise for each band\n",
    "if addnoise==1:\n",
    "    fname_prefix=nnstr+'WiGN_p'+str(noiselevel)\n",
    "    noise=np.random.normal(1,noiselevel/100,(ncase,1))    # only use 2 ratio\n",
    "    rad=np.multiply(rad,noise) \n",
    "else:\n",
    "    fname_prefix=nnstr+'WoGN' \n",
    "     \n",
    "# NN training: Ozone & Cloud Optical Depth   \n",
    "#trainingoption = itrain + 1\n",
    "\n",
    "nnlayer = hiddenlayers\n",
    "            \n",
    "nlayer=len(nnlayer) # number of hidden layers\n",
    "    \n",
    "#create layer string for file name\n",
    "layerstr=''\n",
    "for i in np.arange(nlayer):\n",
    "    if i<nlayer-1:\n",
    "       layerstr=layerstr+str(nnlayer[i])+'X'\n",
    "    else:\n",
    "       layerstr=layerstr+str(nnlayer[i])\n",
    "\n",
    "trainx=np.zeros((ncase,3))\n",
    "trainy=np.zeros((ncase,2))\n",
    "\n",
    "trainx[:,0]=np.cos(np.deg2rad(par[:,0])) # geometry: cos[Solar Zenith Angle]\n",
    "trainx[:,1]=np.log10(par[:,3]) # Irradiance 380\n",
    "trainx[:,2]=np.log10(par[:,4]) # Ratio\n",
    "\n",
    "trainy[:,0]=np.log10(par[:,1]) # Ozone\n",
    "trainy[:,1]=np.log10(par[:,2]) # Cloud. Vol. Frac.\n",
    "\n",
    "net_name='net_'+fname_prefix+'_ozone'+str(nrrs)+'_'+layerstr+'.h5'\t  \n",
    "for i in np.arange(nrrs):\n",
    "    if i==0:\n",
    "       labelparam=['Rrs'+str(band[i])+'nm']\n",
    "    else:\n",
    "       labelparam=np.append(labelparam,['Rrs'+str(band[i])+'nm'],0)\t\t\t  \n",
    "\n",
    "    \n",
    "    \n",
    "#NORMALIZATION IF NEEDED:\n",
    "\n",
    "# ninput=len(trainx[0])\n",
    "# train_in=np.zeros((ninput,2))\n",
    "# for i in range(ninput):\n",
    "#     train_in[i,0]=trainx[:,i].min()\n",
    "#     train_in[i,1]=trainx[:,i].max()\n",
    "# noutput=len(trainy[0])\t\n",
    "# train_out=np.zeros((noutput,2))\n",
    "# for i in range(noutput):\n",
    "#     train_out[i,0]=trainy[:,i].min()\n",
    "#     train_out[i,1]=trainy[:,i].max()\n",
    "    \n",
    "#normalize the trainx and trainy to [-1,1]\t\n",
    "# for i in range(ninput):\n",
    "#     trainx[:,i]=2*(trainx[:,i]-train_in[i,0])/(train_in[i,1]-train_in[i,0])-1\n",
    "# for i in range(noutput):\n",
    "#     trainy[:,i]=2*(trainy[:,i]-train_out[i,0])/(train_out[i,1]-train_out[i,0])-1\n",
    "\t\n",
    "    \n",
    "\n",
    "\n",
    "#Build MLNN\n",
    "mlnn=MLPRegressor(hidden_layer_sizes=nnlayer,\n",
    "     activation='tanh',\n",
    "\t  solver='adam',\n",
    "    \t  batch_size='auto',\n",
    "    \t  learning_rate='adaptive',\n",
    "    \t  learning_rate_init=0.001,\n",
    "    \t  max_iter=1000,\n",
    "    \t  random_state=5,\n",
    "    \t  tol=1.0e-8,\n",
    "    \t  verbose=True,\n",
    "    \t  early_stopping=True,  \n",
    "    \t  validation_fraction=0.1)\n",
    "#traing MLNN\n",
    "mlnn=mlnn.fit(trainx,trainy)\n",
    "    \n",
    "#save trained NN to HDF5\n",
    "# nn_structure=np.zeros([nlayer+2])\n",
    "# nn_structure[0]=ninput\n",
    "# for i in range(nlayer):\n",
    "#     nn_structure[i+1]=nnlayer[i]\n",
    "# nn_structure[nlayer+1]=noutput\n",
    "    \n",
    "# hf = h5py.File('./net/'+net_name,'w')\n",
    "# hf.create_dataset('Layers',dtype='int8',data = nn_structure)\n",
    "# hf.create_dataset('Norm_in',dtype='float64',data = train_in)\n",
    "# hf.create_dataset('Norm_out',dtype='float64',data = train_out)\n",
    "# gw = hf.create_group('Weights')\n",
    "# for i in range(nlayer+1):    \n",
    "#     gw.create_dataset('Layer'+str(i+1),dtype='float64',data=np.transpose(mlnn.coefs_[i]))\n",
    "# gb = hf.create_group('Bias')\n",
    "# for i in range(nlayer+1):\n",
    "#     gb.create_dataset('Layer'+str(i+1),dtype='float64',data=mlnn.intercepts_[i].reshape(int(nn_structure[i+1]),1))\n",
    "# hf.close()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "530d0906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn off numpy warning\n",
    "np.seterr(divide = 'ignore') \n",
    "\n",
    "#make prediction using trained MLNN\n",
    "\n",
    "input_data =  np.loadtxt('NILU_Data/processed_nilu-20140602.txt') \n",
    "\n",
    "nline = len(input_data) #length of input file\n",
    "\n",
    "norm_input=np.zeros((nline,3))\n",
    "\n",
    "norm_input[:,0]=np.cos(np.deg2rad(input_data[:,0])) # geometry: cos[Solar Zenith Angle]\n",
    "norm_input[:,1]=np.log10(input_data[:,1]) # Irradiance 380\n",
    "norm_input[:,2]=np.log10(input_data[:,2]) # Ratio\n",
    "\n",
    "\n",
    "#IF USING NORMALIZATION:\n",
    "\n",
    "# ninput=len(norm_input[0])\n",
    "\n",
    "\n",
    "# norm_extr=np.zeros((ninput,2))\n",
    "# for i in range(ninput):\n",
    "#     norm_extr[i,0]=norm_input[:,i].min()\n",
    "#     norm_extr[i,1]=norm_input[:,i].max()\n",
    "\n",
    "    \n",
    "\n",
    "# for i in range(ninput):\n",
    "#     norm_input[:,i]=2*(norm_input[:,i]-norm_extr[i,0])/(norm_extr[i,1]-norm_extr[i,0])-1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "nnoutput=mlnn.predict(norm_input)\n",
    "\n",
    "\n",
    "\n",
    "#SCALING IT BACK IF NORMALIZATION WAS USED\n",
    "\n",
    "# for i in range(noutput):\n",
    "#     nnoutput[:,i]=(nnoutput[:,i]+1)/2*(train_out[i,1]-train_out[i,0])+train_out[i,0]\n",
    "#     trainy[:,i]=(trainy[:,i]+1)/2*(train_out[i,1]-train_out[i,0])+train_out[i,0]\n",
    "\t\n",
    "nnoutput=10 ** nnoutput\n",
    "trainy=10 ** trainy\n",
    "\n",
    "\n",
    "\n",
    "np.savetxt('nnoutput.txt', nnoutput) \n",
    "\n",
    "\n",
    "#compute average percentage error\n",
    "# diff=(nnoutput-trainy)/trainy*100\n",
    "# ape=np.mean(np.absolute(diff), axis=0)\n",
    "# bias=np.mean(diff, axis=0)\n",
    "    \n",
    "# lim=np.amax(trainy,axis=0)*1.2 #set limit\n",
    "# r2=np.zeros(noutput)\n",
    "# if ncase>nplotsample:\n",
    "#    idx=random.sample(list(range(ncase)),nplotsample)\n",
    "# else:\n",
    "#    idx=random.sample(list(range(ncase)),int(ncase/2))\n",
    "# if noutput<5:\n",
    "#    plt.figure(figsize=(18,5),dpi=150)\n",
    "# else:\n",
    "#    plt.figure(figsize=(18,9),dpi=150)\n",
    "# for i in np.arange(noutput):\n",
    "#    r2[i]=r2_score(trainy[:,i],nnoutput[:,i])\n",
    "#    print(r2[i])\n",
    "#    plt.subplot(2,4,i+1)           \n",
    "#    plt.scatter(trainy[idx,i],nnoutput[idx,i],s=2,c='red')\n",
    "#    plt.xlim(0, lim[i])\n",
    "#    plt.ylim(0, lim[i])\n",
    "#    plt.xlabel('Model '+labelparam[i]) #original line:   plt.xlabel('Model '+labelparam[i])\n",
    "#    plt.ylabel('MLNN '+labelparam[i])  #original line: plt.ylabel('MLNN '+labelparam[i])\n",
    "#    plt.plot([0,lim[i]],[0,lim[i]],'k')\n",
    "#    plt.text(lim[i]*0.05,lim[i]*0.94,('R$^2$ = %0.3f' % (r2[i])))\n",
    "#    plt.text(lim[i]*0.05,lim[i]*0.88,('APE = %0.2f' % (ape[i])+'%'))\n",
    "#    plt.text(lim[i]*0.05,lim[i]*0.82,('Bias = %0.2f' % (bias[i])+'%'))\n",
    "# plt.tight_layout()\n",
    "# plt.savefig(output_path+net_name+'_Training.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90909a31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
